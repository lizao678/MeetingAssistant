from fastapi import FastAPI, WebSocket, WebSocketDisconnect, Request, HTTPException
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from starlette.status import HTTP_422_UNPROCESSABLE_ENTITY
from pydantic_settings import BaseSettings
from pydantic import BaseModel, Field
from funasr import AutoModel
import numpy as np
import soundfile as sf
import argparse
import uvicorn
from urllib.parse import parse_qs
import os
import re
from modelscope.pipelines import pipeline
from modelscope.utils.constant import Tasks
from loguru import logger
import sys
import json
import traceback
import time

# --- æ—¥å¿—é…ç½® (æ— æ”¹åŠ¨) ---
logger.remove()
log_format = "{time:YYYY-MM-DD HH:mm:ss} [{level}] {file}:{line} - {message}"
logger.add(sys.stdout, format=log_format, level="DEBUG", filter=lambda record: record["level"].no < 40)
logger.add(sys.stderr, format=log_format, level="ERROR", filter=lambda record: record["level"].no >= 40)

# --- å…¨å±€é…ç½® (æ— æ”¹åŠ¨) ---
class Config(BaseSettings):
    sv_thr: float = Field(0.42, description="Speaker verification threshold for diarization") # æé«˜é˜ˆå€¼ï¼Œå¹³è¡¡ç²¾åº¦å’Œå¬å›
    chunk_size_ms: int = Field(300, description="Chunk size in milliseconds")
    sample_rate: int = Field(16000, description="Sample rate in Hz")
    bit_depth: int = Field(16, description="Bit depth")
    channels: int = Field(1, description="Number of audio channels")
    avg_logprob_thr: float = Field(-0.25, description="average logprob threshold")
    # æ–°å¢è¯´è¯äººè¯†åˆ«ç›¸å…³é…ç½®
    min_audio_length_ms: int = Field(800, description="Minimum audio length for speaker verification in milliseconds")  # é™ä½æœ€å°é•¿åº¦è¦æ±‚
    max_audio_length_ms: int = Field(5000, description="Maximum audio length for speaker verification in milliseconds")
    speaker_continuity_threshold: int = Field(3, description="Number of consecutive segments to confirm speaker identity")
    confidence_decay: float = Field(0.95, description="Confidence decay factor for speaker continuity")

config = Config()

# --- æ–‡æœ¬æ ¼å¼åŒ–ç›¸å…³ (æ— æ”¹åŠ¨) ---
# ... (æ­¤å¤„çœç•¥ emo_dict, event_dict, emoji_dict, lang_dict ç­‰å­—å…¸å’Œæ ¼å¼åŒ–å‡½æ•°ï¼Œä¸æ‚¨åŸä»£ç ç›¸åŒ)
emo_dict = {
    "<|HAPPY|>": "ğŸ˜Š", "<|SAD|>": "ğŸ˜”", "<|ANGRY|>": "ğŸ˜¡", "<|NEUTRAL|>": "",
    "<|FEARFUL|>": "ğŸ˜°", "<|DISGUSTED|>": "ğŸ¤¢", "<|SURPRISED|>": "ğŸ˜®",
}
event_dict = {
    "<|BGM|>": "ğŸ¼", "<|Speech|>": "", "<|Applause|>": "ğŸ‘", "<|Laughter|>": "ğŸ˜€",
    "<|Cry|>": "ğŸ˜­", "<|Sneeze|>": "ğŸ¤§", "<|Breath|>": "", "<|Cough|>": "ğŸ¤§",
}
emoji_dict = {
    "<|nospeech|><|Event_UNK|>": "â“", "<|zh|>": "", "<|en|>": "", "<|yue|>": "",
    "<|ja|>": "", "<|ko|>": "", "<|nospeech|>": "", "<|HAPPY|>": "ğŸ˜Š", "<|SAD|>": "ğŸ˜”",
    "<|ANGRY|>": "ğŸ˜¡", "<|NEUTRAL|>": "", "<|BGM|>": "ğŸ¼", "<|Speech|>": "",
    "<|Applause|>": "ğŸ‘", "<|Laughter|>": "ğŸ˜€", "<|FEARFUL|>": "ğŸ˜°",
    "<|DISGUSTED|>": "ğŸ¤¢", "<|SURPRISED|>": "ğŸ˜®", "<|Cry|>": "ğŸ˜­", "<|EMO_UNKNOWN|>": "",
    "<|Sneeze|>": "ğŸ¤§", "<|Breath|>": "", "<|Cough|>": "ğŸ˜·", "<|Sing|>": "",
    "<|Speech_Noise|>": "", "<|withitn|>": "", "<|woitn|>": "", "<|GBG|>": "", "<|Event_UNK|>": "",
}
lang_dict = {
    "<|zh|>": "<|lang|>", "<|en|>": "<|lang|>", "<|yue|>": "<|lang|>",
    "<|ja|>": "<|lang|>", "<|ko|>": "<|lang|>", "<|nospeech|>": "<|lang|>",
}
emo_set = {"ğŸ˜Š", "ğŸ˜”", "ğŸ˜¡", "ğŸ˜°", "ğŸ¤¢", "ğŸ˜®"}
event_set = {"ğŸ¼", "ğŸ‘", "ğŸ˜€", "ğŸ˜­", "ğŸ¤§", "ğŸ˜·"}

def format_str(s):
    for sptk in emoji_dict:
        s = s.replace(sptk, emoji_dict[sptk])
    return s

def format_str_v2(s):
    sptk_dict = {}
    for sptk in emoji_dict:
        sptk_dict[sptk] = s.count(sptk)
        s = s.replace(sptk, "")
    emo = "<|NEUTRAL|>"
    for e in emo_dict:
        if sptk_dict[e] > sptk_dict[emo]:
            emo = e
    for e in event_dict:
        if sptk_dict[e] > 0:
            s = event_dict[e] + s
    s = s + emo_dict[emo]
    for emoji in emo_set.union(event_set):
        s = s.replace(" " + emoji, emoji)
        s = s.replace(emoji + " ", emoji)
    return s.strip()

def format_str_v3(s):
    def get_emo(s):
        return s[-1] if s[-1] in emo_set else None
    def get_event(s):
        return s[0] if s[0] in event_set else None
    s = s.replace("<|nospeech|><|Event_UNK|>", "â“")
    for lang in lang_dict:
        s = s.replace(lang, "<|lang|>")
    s_list = [format_str_v2(s_i).strip(" ") for s_i in s.split("<|lang|>")]
    new_s = " " + s_list[0]
    cur_ent_event = get_event(new_s)
    for i in range(1, len(s_list)):
        if len(s_list[i]) == 0:
            continue
        if get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) != None:
            s_list[i] = s_list[i][1:]
        cur_ent_event = get_event(s_list[i])
        if get_emo(s_list[i]) != None and get_emo(s_list[i]) == get_emo(new_s):
            new_s = new_s[:-1]
        new_s += s_list[i].strip().lstrip()
    new_s = new_s.replace("The.", " ")
    return new_s.strip()

def contains_chinese_english_number(s: str) -> bool:
    return bool(re.search(r'[\u4e00-\u9fffA-Za-z0-9]', s))


# --- æ¨¡å‹åŠ è½½ (æ— æ”¹åŠ¨) ---
sv_pipeline = pipeline(
    task='speaker-verification',
    model='iic/speech_eres2net_large_sv_zh-cn_3dspeaker_16k',
    model_revision='v1.0.0'
)
model_asr = AutoModel(
    model="iic/SenseVoiceSmall",
    trust_remote_code=True,
    remote_code="./model.py",
    device="cuda:0",
    disable_update=True
)
model_vad = AutoModel(
    model="fsmn-vad",
    model_revision="v2.0.4",
    disable_pbar=True,
    max_end_silence_time=500,
    disable_update=True,
)


# --- ã€å·²åºŸå¼ƒã€‘æ—§çš„å…¨å±€è¯´è¯äººæ³¨å†Œé€»è¾‘ ---
# reg_spks_files = [
#     "speaker/speaker1_a_cn_16k.wav"
# ]
# def reg_spk_init(files): ...
# reg_spks = reg_spk_init(reg_spks_files)


# --- ã€æ”¹è¿›ã€‘åœ¨çº¿è¯´è¯äººæ—¥å¿—å‡½æ•° ---
def check_audio_quality(audio_segment, sample_rate=16000):
    """
    æ£€æŸ¥éŸ³é¢‘è´¨é‡ï¼Œç¡®ä¿é€‚åˆè¿›è¡Œè¯´è¯äººè¯†åˆ«
    
    Args:
        audio_segment (np.ndarray): éŸ³é¢‘ç‰‡æ®µ
        sample_rate (int): é‡‡æ ·ç‡
        
    Returns:
        bool: éŸ³é¢‘è´¨é‡æ˜¯å¦åˆæ ¼
    """
    if len(audio_segment) == 0:
        return False
    
    # æ£€æŸ¥éŸ³é¢‘é•¿åº¦
    duration_ms = len(audio_segment) / sample_rate * 1000
    if duration_ms < config.min_audio_length_ms or duration_ms > config.max_audio_length_ms:
        logger.debug(f"Audio duration {duration_ms:.1f}ms not suitable for speaker verification")
        return False
    
    # æ£€æŸ¥éŸ³é¢‘èƒ½é‡ - é™ä½è¦æ±‚
    energy = np.mean(np.abs(audio_segment))
    if energy < 0.005:  # é™ä½èƒ½é‡é˜ˆå€¼ï¼ˆåŸå€¼ï¼š0.01ï¼‰
        logger.debug(f"Audio energy {energy:.4f} too low for speaker verification")
        return False
    
    # æ£€æŸ¥éŸ³é¢‘æ–¹å·®ï¼ˆé¿å…é™éŸ³æˆ–å™ªå£°ï¼‰- é™ä½è¦æ±‚
    variance = np.var(audio_segment)
    if variance < 0.0005:  # é™ä½æ–¹å·®é˜ˆå€¼ï¼ˆåŸå€¼ï¼š0.001ï¼‰
        logger.debug(f"Audio variance {variance:.6f} too low for speaker verification")
        return False
    
    return True

def diarize_speaker_online_improved(audio_segment, speaker_gallery, speaker_counter, sv_thr, 
                                   speaker_history=None, current_speaker=None):
    """
    æ”¹è¿›çš„åœ¨çº¿è¯´è¯äººæ—¥å¿—åˆ†æå‡½æ•°
    
    Args:
        audio_segment (np.ndarray): å½“å‰çš„è¯­éŸ³ç‰‡æ®µ
        speaker_gallery (dict): å½“å‰ä¼šè¯çš„å£°çº¹åº“ {speaker_id: reference_audio}
        speaker_counter (int): å½“å‰ä¼šè¯çš„è¯´è¯äººè®¡æ•°å™¨
        sv_thr (float): å£°çº¹æ¯”å¯¹çš„ç›¸ä¼¼åº¦é˜ˆå€¼
        speaker_history (list): è¯´è¯äººå†å²è®°å½• [(speaker_id, confidence, timestamp), ...]
        current_speaker (str): å½“å‰æ´»è·ƒçš„è¯´è¯äºº
        
    Returns:
        tuple: (è¯†åˆ«å‡ºçš„speaker_id, æ›´æ–°åçš„speaker_gallery, æ›´æ–°åçš„speaker_counter, 
                æ›´æ–°åçš„speaker_history, æ›´æ–°åçš„current_speaker)
    """
    import time
    
    # åˆå§‹åŒ–å†å²è®°å½•
    if speaker_history is None:
        speaker_history = []
    current_time = time.time()
    
    # æ£€æŸ¥éŸ³é¢‘è´¨é‡
    if not check_audio_quality(audio_segment):
        # éŸ³é¢‘è´¨é‡ä¸åˆæ ¼ï¼Œä½¿ç”¨å½“å‰è¯´è¯äººæˆ–é»˜è®¤å€¼
        if current_speaker:
            logger.debug(f"Using current speaker '{current_speaker}' due to poor audio quality")
            return current_speaker, speaker_gallery, speaker_counter, speaker_history, current_speaker
        else:
            return "å‘è¨€äºº", speaker_gallery, speaker_counter, speaker_history, current_speaker
    
    # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªè¯´è¯äºº
    if not speaker_gallery:
        speaker_counter += 1
        speaker_id = f"å‘è¨€äºº{speaker_counter}"
        speaker_gallery[speaker_id] = audio_segment
        speaker_history.append((speaker_id, 1.0, current_time))
        logger.info(f"First speaker detected. Assigning ID: {speaker_id}")
        return speaker_id, speaker_gallery, speaker_counter, speaker_history, speaker_id
    
    # ä¸å£°çº¹åº“ä¸­çš„æ‰€æœ‰è¯´è¯äººè¿›è¡Œæ¯”å¯¹
    best_score = -1.0
    identified_speaker = None
    scores = {}
    
    for spk_id, ref_audio in speaker_gallery.items():
        try:
            res_sv = sv_pipeline([audio_segment, ref_audio])
            score = res_sv["score"]
            scores[spk_id] = score
            logger.debug(f"Comparing with '{spk_id}', score: {score:.4f}")
            if score > best_score:
                best_score = score
                identified_speaker = spk_id
        except Exception as e:
            logger.error(f"Error during speaker comparison with {spk_id}: {e}")
            continue
    
    # åŠ¨æ€é˜ˆå€¼è°ƒæ•´ï¼šåŸºäºå†å²è®°å½•å’Œå½“å‰è¯´è¯äºº
    dynamic_threshold = sv_thr
    
    # å¦‚æœå½“å‰æœ‰æ´»è·ƒè¯´è¯äººï¼Œé™ä½åˆ‡æ¢é˜ˆå€¼
    if current_speaker and current_speaker in scores:
        current_score = scores[current_speaker]
        # å¦‚æœå½“å‰è¯´è¯äººåˆ†æ•°è¾ƒé«˜ï¼Œæé«˜åˆ‡æ¢é˜ˆå€¼
        if current_score > sv_thr * 0.8:
            dynamic_threshold = sv_thr * 1.1  # é™ä½é˜ˆå€¼æå‡å€æ•°ï¼ˆåŸå€¼ï¼š1.2ï¼‰
            logger.debug(f"Raised threshold to {dynamic_threshold:.3f} for current speaker '{current_speaker}'")
    
    # è¿ç»­æ€§åˆ¤æ–­ - é™ä½ä¸¥æ ¼ç¨‹åº¦
    if current_speaker and identified_speaker == current_speaker and best_score >= sv_thr * 0.6:  # é™ä½è¿ç»­æ€§é˜ˆå€¼ï¼ˆåŸå€¼ï¼š0.7ï¼‰
        # è¿ç»­è¯´è¯ï¼Œä¿æŒå½“å‰è¯´è¯äºº
        confidence = min(1.0, best_score)
        speaker_history.append((current_speaker, confidence, current_time))
        logger.info(f"Speaker continuity confirmed: '{current_speaker}' (score: {best_score:.4f})")
        return current_speaker, speaker_gallery, speaker_counter, speaker_history, current_speaker
    
    # æ£€æŸ¥æ˜¯å¦åŒ¹é…åˆ°å·²æœ‰è¯´è¯äºº
    if best_score >= dynamic_threshold:
        # åŒ¹é…åˆ°å·²æœ‰è¯´è¯äºº
        confidence = min(1.0, best_score)
        speaker_history.append((identified_speaker, confidence, current_time))
        logger.info(f"Matched existing speaker '{identified_speaker}' with score {best_score:.4f}")
        return identified_speaker, speaker_gallery, speaker_counter, speaker_history, identified_speaker
    
    # æ£€æŸ¥æ˜¯å¦åº”è¯¥åˆ›å»ºæ–°è¯´è¯äºº - é™ä½åˆ›å»ºæ–°è¯´è¯äººçš„é—¨æ§›
    # åªæœ‰å½“æ‰€æœ‰åˆ†æ•°éƒ½æ˜æ˜¾ä½äºé˜ˆå€¼æ—¶æ‰åˆ›å»ºæ–°è¯´è¯äºº
    all_scores_low = all(score < sv_thr * 0.7 for score in scores.values())  # é™ä½é—¨æ§›ï¼ˆåŸå€¼ï¼š0.8ï¼‰
    
    if all_scores_low:
        # åˆ›å»ºæ–°è¯´è¯äºº
        speaker_counter += 1
        new_speaker_id = f"å‘è¨€äºº{speaker_counter}"
        speaker_gallery[new_speaker_id] = audio_segment
        speaker_history.append((new_speaker_id, 0.8, current_time))
        logger.info(f"New speaker detected (all scores < {sv_thr * 0.7:.3f}). Assigning ID: {new_speaker_id}")
        return new_speaker_id, speaker_gallery, speaker_counter, speaker_history, new_speaker_id
    else:
        # åˆ†æ•°ä¸å¤Ÿé«˜ä½†ä¹Ÿä¸å¤Ÿä½ï¼Œä½¿ç”¨æœ€ä½³åŒ¹é…
        best_speaker = max(scores.items(), key=lambda x: x[1])[0]
        confidence = min(0.7, scores[best_speaker])
        speaker_history.append((best_speaker, confidence, current_time))
        logger.info(f"Using best match '{best_speaker}' with moderate confidence {scores[best_speaker]:.4f}")
        return best_speaker, speaker_gallery, speaker_counter, speaker_history, best_speaker

# ä¿ç•™åŸå‡½æ•°åä»¥å…¼å®¹ç°æœ‰ä»£ç 
def diarize_speaker_online(audio_segment, speaker_gallery, speaker_counter, sv_thr):
    """
    å…¼å®¹æ€§åŒ…è£…å‡½æ•°ï¼Œè°ƒç”¨æ”¹è¿›çš„è¯´è¯äººè¯†åˆ«ç®—æ³•
    """
    result = diarize_speaker_online_improved(
        audio_segment, speaker_gallery, speaker_counter, sv_thr, 
        speaker_history=None, current_speaker=None
    )
    return result[0], result[1], result[2]  # åªè¿”å›å‰ä¸‰ä¸ªå€¼ä»¥ä¿æŒå…¼å®¹æ€§


def asr(audio, lang, cache, use_itn=False):
    start_time = time.time()
    result = model_asr.generate(
        input=audio,
        cache=cache,
        language=lang.strip(),
        use_itn=use_itn,
        batch_size_s=60,
    )
    end_time = time.time()
    elapsed_time = end_time - start_time
    logger.debug(f"asr elapsed: {elapsed_time * 1000:.2f} milliseconds")
    return result


# --- FastAPI åº”ç”¨è®¾ç½® (æ— æ”¹åŠ¨) ---
app = FastAPI()
app.add_middleware(
    CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"],
)
@app.exception_handler(Exception)
async def custom_exception_handler(request: Request, exc: Exception):
    # ... (æ­¤å¤„çœç•¥å¼‚å¸¸å¤„ç†ä»£ç , ä¸æ‚¨åŸä»£ç ç›¸åŒ)
    logger.error("Exception occurred", exc_info=True)
    if isinstance(exc, HTTPException):
        status_code = exc.status_code
        message = exc.detail
    elif isinstance(exc, RequestValidationError):
        status_code = HTTP_422_UNPROCESSABLE_ENTITY
        message = "Validation error: " + str(exc.errors())
    else:
        status_code = 500
        message = "Internal server error: " + str(exc)
    
    return JSONResponse(
        status_code=status_code,
        content=TranscriptionResponse(
            code=status_code,
            msg=message, # ä¿®æ­£: åŸä»£ç æ­¤å¤„ä¸ºinfo, æ”¹ä¸ºmsgä»¥åŒ¹é…æ¨¡å‹å®šä¹‰
            data=""
        ).model_dump()
    )

class TranscriptionResponse(BaseModel):
    code: int
    msg: str # ä¿æŒä¸åŸå®šä¹‰ä¸€è‡´
    data: str

# ä¿®æ­£: ä¸Šæ–¹å¼‚å¸¸å¤„ç†å™¨ä¸­çš„contentåº”ä¸æ¨¡å‹å®šä¹‰åŒ¹é…
# class TranscriptionResponse(BaseModel):
#     code: int
#     msg: str  # å‡è®¾æ¨¡å‹å®šä¹‰æ˜¯msg
#     data: str


# --- ã€ä¿®æ”¹åã€‘WebSocket ç«¯ç‚¹ ---
@app.websocket("/ws/transcribe")
async def websocket_endpoint(websocket: WebSocket):
    try:
        query_params = parse_qs(websocket.scope['query_string'].decode())
        sv = query_params.get('sv', ['false'])[0].lower() in ['true', '1', 't', 'y', 'yes']
        lang = query_params.get('lang', ['auto'])[0].lower()
        
        await websocket.accept()

        # --- ä¸ºæ¯ä¸ªä¼šè¯åˆå§‹åŒ–çŠ¶æ€ ---
        chunk_size = int(config.chunk_size_ms * config.sample_rate / 1000)
        audio_buffer = np.array([], dtype=np.float32)
        audio_vad = np.array([], dtype=np.float32)
        cache_vad = {}
        cache_asr = {}
        last_vad_beg = last_vad_end = -1
        offset = 0
        
        # ã€æ”¹è¿›ã€‘ä¸ºå½“å‰è¿æ¥åˆ›å»ºç‹¬ç«‹çš„å£°çº¹åº“å’Œè®¡æ•°å™¨
        speaker_gallery = {}  # key: "å‘è¨€äºº1", value: np.ndarray (å£°çº¹éŸ³é¢‘)
        speaker_counter = 0
        speaker_history = []  # è¯´è¯äººå†å²è®°å½•
        current_speaker = None  # å½“å‰æ´»è·ƒçš„è¯´è¯äºº
        
        buffer = b""
        while True:
            data = await websocket.receive_bytes()
            buffer += data
            if len(buffer) < 2:
                continue
                
            audio_buffer = np.append(
                audio_buffer, 
                np.frombuffer(buffer[:len(buffer) - (len(buffer) % 2)], dtype=np.int16).astype(np.float32) / 32767.0
            )
            buffer = buffer[len(buffer) - (len(buffer) % 2):]
   
            while len(audio_buffer) >= chunk_size:
                chunk = audio_buffer[:chunk_size]
                audio_buffer = audio_buffer[chunk_size:]
                audio_vad = np.append(audio_vad, chunk)
                
                res = model_vad.generate(input=chunk, cache=cache_vad, is_final=False, chunk_size=config.chunk_size_ms)
                
                if len(res[0]["value"]):
                    for segment in res[0]["value"]:
                        if segment[0] > -1: last_vad_beg = segment[0]
                        if segment[1] > -1: last_vad_end = segment[1]
                        if last_vad_beg > -1 and last_vad_end > -1:
                            beg = int((last_vad_beg - offset) * config.sample_rate / 1000)
                            end = int((last_vad_end - offset) * config.sample_rate / 1000)
                            
                            segment_audio = audio_vad[beg:end]
                            logger.info(f"[vad segment] audio_len: {len(segment_audio)}")

                            speaker_id = "å‘è¨€äºº" # é»˜è®¤ID
                            if sv and len(segment_audio) > 0:
                                # ä½¿ç”¨æ”¹è¿›çš„è¯´è¯äººè¯†åˆ«ç®—æ³•
                                speaker_id, speaker_gallery, speaker_counter, speaker_history, current_speaker = diarize_speaker_online_improved(
                                    segment_audio, speaker_gallery, speaker_counter, config.sv_thr,
                                    speaker_history, current_speaker
                                )
                            
                            # 2. è¿›è¡Œè¯­éŸ³è¯†åˆ«
                            result = asr(segment_audio, lang.strip(), cache_asr, True)
                            logger.info(f"asr response: {result}")
                            
                            audio_vad = audio_vad[end:]
                            offset = last_vad_end # ä¿®æ­£offsetæ›´æ–°é€»è¾‘
                            last_vad_beg = last_vad_end = -1
                            
                            if result is not None and contains_chinese_english_number(result[0]['text']):
                                formatted_text = format_str_v3(result[0]['text'])
                                
                                # 3. åˆå¹¶è¯´è¯äººIDå’Œè¯†åˆ«æ–‡æœ¬
                                final_data = f"[{speaker_id}]: {formatted_text}"
                                
                                response = TranscriptionResponse(
                                    code=0,
                                    msg=json.dumps(result[0], ensure_ascii=False),
                                    data=final_data
                                )
                                await websocket.send_json(response.model_dump())
                                
    except WebSocketDisconnect:
        logger.info("WebSocket disconnected")
    except Exception as e:
        logger.error(f"Unexpected error: {e}\nCall stack:\n{traceback.format_exc()}")
        await websocket.close()
    finally:
        logger.info("Cleaned up resources after WebSocket disconnect")


# --- æœåŠ¡å¯åŠ¨ (æ— æ”¹åŠ¨) ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run the FastAPI app with a specified port.")
    parser.add_argument('--port', type=int, default=27000, help='Port number to run the FastAPI app on.')
    parser.add_argument('--certfile', type=str, default='path_to_your_certfile', help='SSL certificate file')
    parser.add_argument('--keyfile', type=str, default='path_to_your_keyfile', help='SSL key file')
    args = parser.parse_args()

    uvicorn.run(app, host="0.0.0.0", port=args.port)