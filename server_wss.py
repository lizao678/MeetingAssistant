from fastapi import FastAPI, WebSocket, WebSocketDisconnect, Request, HTTPException
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from starlette.status import HTTP_422_UNPROCESSABLE_ENTITY
from pydantic_settings import BaseSettings
from pydantic import BaseModel, Field
from funasr import AutoModel
import numpy as np
import soundfile as sf
import argparse
import uvicorn
from urllib.parse import parse_qs
import os
import re
from modelscope.pipelines import pipeline
from modelscope.utils.constant import Tasks
from loguru import logger
import sys
import json
import traceback
import time
import asyncio
from concurrent.futures import ThreadPoolExecutor
from contextlib import asynccontextmanager
from collections import deque
from typing import List, Optional

# --- é«˜æ•ˆéŸ³é¢‘ç¼“å†²åŒºç±» ---
class AudioBuffer:
    """
    é«˜æ•ˆçš„éŸ³é¢‘ç¼“å†²åŒºå®ç°ï¼Œä½¿ç”¨dequeé¿å…np.appendçš„æ€§èƒ½é—®é¢˜
    """
    def __init__(self, max_size: Optional[int] = None, dtype=np.float32):
        self.chunks = deque(maxlen=max_size)
        self.dtype = dtype
        self._total_length = 0
        
    def append(self, data: np.ndarray):
        """æ·»åŠ æ–°çš„éŸ³é¢‘æ•°æ®"""
        if len(data) > 0:
            self.chunks.append(data.astype(self.dtype))
            self._total_length += len(data)
    
    def get_data(self, start_idx: int = 0, length: Optional[int] = None) -> np.ndarray:
        """è·å–æŒ‡å®šèŒƒå›´çš„éŸ³é¢‘æ•°æ®"""
        if not self.chunks:
            return np.array([], dtype=self.dtype)
        
        # åˆå¹¶æ‰€æœ‰chunks
        combined = np.concatenate(list(self.chunks))
        
        if length is None:
            return combined[start_idx:]
        else:
            end_idx = start_idx + length
            return combined[start_idx:min(end_idx, len(combined))]
    
    def pop_front(self, length: int) -> np.ndarray:
        """ä»å‰é¢å¼¹å‡ºæŒ‡å®šé•¿åº¦çš„æ•°æ®"""
        if not self.chunks:
            return np.array([], dtype=self.dtype)
        
        result_data = []
        remaining_length = length
        
        while remaining_length > 0 and self.chunks:
            chunk = self.chunks[0]
            
            if len(chunk) <= remaining_length:
                # æ•´ä¸ªchunkéƒ½è¦è¢«å¼¹å‡º
                result_data.append(self.chunks.popleft())
                remaining_length -= len(chunk)
                self._total_length -= len(chunk)
            else:
                # åªå¼¹å‡ºchunkçš„ä¸€éƒ¨åˆ†
                result_data.append(chunk[:remaining_length])
                # æ›´æ–°ç¬¬ä¸€ä¸ªchunk
                self.chunks[0] = chunk[remaining_length:]
                self._total_length -= remaining_length
                remaining_length = 0
        
        return np.concatenate(result_data) if result_data else np.array([], dtype=self.dtype)
    
    def __len__(self) -> int:
        """è¿”å›ç¼“å†²åŒºä¸­çš„æ€»æ ·æœ¬æ•°"""
        return self._total_length
    
    def clear(self):
        """æ¸…ç©ºç¼“å†²åŒº"""
        self.chunks.clear()
        self._total_length = 0
    
    def slice_and_keep_rest(self, start_idx: int, end_idx: int) -> np.ndarray:
        """åˆ‡ç‰‡æ•°æ®å¹¶ä¿ç•™å‰©ä½™éƒ¨åˆ†"""
        if not self.chunks:
            return np.array([], dtype=self.dtype)
        
        # è·å–æ‰€æœ‰æ•°æ®
        all_data = np.concatenate(list(self.chunks))
        
        # æå–æŒ‡å®šèŒƒå›´çš„æ•°æ®
        sliced_data = all_data[start_idx:end_idx]
        
        # ä¿ç•™å‰©ä½™æ•°æ®
        remaining_data = all_data[end_idx:]
        
        # é‡æ–°è®¾ç½®ç¼“å†²åŒº
        self.clear()
        if len(remaining_data) > 0:
            self.append(remaining_data)
        
        return sliced_data

class CircularAudioBuffer:
    """
    å¾ªç¯éŸ³é¢‘ç¼“å†²åŒºï¼Œç”¨äºVADå¤„ç†ï¼Œé¿å…é¢‘ç¹çš„å†…å­˜åˆ†é…
    """
    def __init__(self, max_samples: int, dtype=np.float32):
        self.buffer = np.zeros(max_samples, dtype=dtype)
        self.max_samples = max_samples
        self.write_pos = 0
        self.read_pos = 0
        self.size = 0
        self.dtype = dtype
    
    def append(self, data: np.ndarray):
        """æ·»åŠ æ•°æ®åˆ°å¾ªç¯ç¼“å†²åŒº"""
        data = data.astype(self.dtype)
        data_len = len(data)
        
        # å¦‚æœæ•°æ®å¤ªå¤§ï¼Œåªä¿ç•™æœ€åçš„éƒ¨åˆ†
        if data_len >= self.max_samples:
            self.buffer[:] = data[-self.max_samples:]
            self.write_pos = 0
            self.read_pos = 0
            self.size = self.max_samples
            return
        
        # è®¡ç®—å¯ç”¨ç©ºé—´
        available_space = self.max_samples - self.size
        
        if data_len <= available_space:
            # æœ‰è¶³å¤Ÿç©ºé—´ï¼Œç›´æ¥æ·»åŠ 
            end_pos = self.write_pos + data_len
            if end_pos <= self.max_samples:
                self.buffer[self.write_pos:end_pos] = data
            else:
                # éœ€è¦ç¯ç»•
                split_point = self.max_samples - self.write_pos
                self.buffer[self.write_pos:] = data[:split_point]
                self.buffer[:end_pos - self.max_samples] = data[split_point:]
            
            self.write_pos = end_pos % self.max_samples
            self.size += data_len
        else:
            # ç©ºé—´ä¸è¶³ï¼Œè¦†ç›–æ—§æ•°æ®
            overwrite_count = data_len - available_space
            
            # å…ˆæ·»åŠ åˆ°å¯ç”¨ç©ºé—´
            if available_space > 0:
                self.append(data[:available_space])
            
            # ç„¶åè¦†ç›–æ—§æ•°æ®
            self.read_pos = (self.read_pos + overwrite_count) % self.max_samples
            self.size -= overwrite_count
            
            # æ·»åŠ å‰©ä½™æ•°æ®
            self.append(data[available_space:])
    
    def get_range(self, start_offset: int, length: int) -> np.ndarray:
        """è·å–æŒ‡å®šèŒƒå›´çš„æ•°æ®"""
        if length <= 0 or start_offset >= self.size:
            return np.array([], dtype=self.dtype)
        
        # è°ƒæ•´é•¿åº¦ä»¥ä¸è¶…è¿‡å¯ç”¨æ•°æ®
        actual_length = min(length, self.size - start_offset)
        result = np.zeros(actual_length, dtype=self.dtype)
        
        start_pos = (self.read_pos + start_offset) % self.max_samples
        end_pos = start_pos + actual_length
        
        if end_pos <= self.max_samples:
            result[:] = self.buffer[start_pos:end_pos]
        else:
            # éœ€è¦ç¯ç»•è¯»å–
            first_part_len = self.max_samples - start_pos
            result[:first_part_len] = self.buffer[start_pos:]
            result[first_part_len:] = self.buffer[:actual_length - first_part_len]
        
        return result
    
    def pop_front(self, length: int) -> np.ndarray:
        """ä»å‰é¢å¼¹å‡ºæ•°æ®"""
        if length <= 0 or self.size == 0:
            return np.array([], dtype=self.dtype)
        
        actual_length = min(length, self.size)
        result = self.get_range(0, actual_length)
        
        # æ›´æ–°è¯»å–ä½ç½®å’Œå¤§å°
        self.read_pos = (self.read_pos + actual_length) % self.max_samples
        self.size -= actual_length
        
        return result
    
    def __len__(self) -> int:
        return self.size
    
    def clear(self):
        """æ¸…ç©ºç¼“å†²åŒº"""
        self.write_pos = 0
        self.read_pos = 0
        self.size = 0

# --- çº¿ç¨‹æ± é…ç½® ---
thread_pool_executor = ThreadPoolExecutor(max_workers=4, thread_name_prefix="model_inference")

# --- åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç† ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    # åº”ç”¨å¯åŠ¨
    logger.info("Application starting up...")
    yield
    # åº”ç”¨å…³é—­
    logger.info("Application shutting down...")
    thread_pool_executor.shutdown(wait=True)
    logger.info("Thread pool executor shut down")

# --- æ—¥å¿—é…ç½® (æ— æ”¹åŠ¨) ---
logger.remove()
log_format = "{time:YYYY-MM-DD HH:mm:ss} [{level}] {file}:{line} - {message}"
logger.add(sys.stdout, format=log_format, level="DEBUG", filter=lambda record: record["level"].no < 40)
logger.add(sys.stderr, format=log_format, level="ERROR", filter=lambda record: record["level"].no >= 40)

# --- å…¨å±€é…ç½® (æ— æ”¹åŠ¨) ---
class Config(BaseSettings):
    sv_thr: float = Field(0.42, description="Speaker verification threshold for diarization") # æé«˜é˜ˆå€¼ï¼Œå¹³è¡¡ç²¾åº¦å’Œå¬å›
    chunk_size_ms: int = Field(300, description="Chunk size in milliseconds")
    sample_rate: int = Field(16000, description="Sample rate in Hz")
    bit_depth: int = Field(16, description="Bit depth")
    channels: int = Field(1, description="Number of audio channels")
    avg_logprob_thr: float = Field(-0.25, description="average logprob threshold")
    # æ–°å¢è¯´è¯äººè¯†åˆ«ç›¸å…³é…ç½®
    min_audio_length_ms: int = Field(800, description="Minimum audio length for speaker verification in milliseconds")  # é™ä½æœ€å°é•¿åº¦è¦æ±‚
    max_audio_length_ms: int = Field(5000, description="Maximum audio length for speaker verification in milliseconds")
    speaker_continuity_threshold: int = Field(3, description="Number of consecutive segments to confirm speaker identity")
    confidence_decay: float = Field(0.95, description="Confidence decay factor for speaker continuity")

config = Config()

# --- æ–‡æœ¬æ ¼å¼åŒ–ç›¸å…³ (æ— æ”¹åŠ¨) ---
# ... (æ­¤å¤„çœç•¥ emo_dict, event_dict, emoji_dict, lang_dict ç­‰å­—å…¸å’Œæ ¼å¼åŒ–å‡½æ•°ï¼Œä¸æ‚¨åŸä»£ç ç›¸åŒ)
emo_dict = {
    "<|HAPPY|>": "ğŸ˜Š", "<|SAD|>": "ğŸ˜”", "<|ANGRY|>": "ğŸ˜¡", "<|NEUTRAL|>": "",
    "<|FEARFUL|>": "ğŸ˜°", "<|DISGUSTED|>": "ğŸ¤¢", "<|SURPRISED|>": "ğŸ˜®",
}
event_dict = {
    "<|BGM|>": "ğŸ¼", "<|Speech|>": "", "<|Applause|>": "ğŸ‘", "<|Laughter|>": "ğŸ˜€",
    "<|Cry|>": "ğŸ˜­", "<|Sneeze|>": "ğŸ¤§", "<|Breath|>": "", "<|Cough|>": "ğŸ¤§",
}
emoji_dict = {
    "<|nospeech|><|Event_UNK|>": "â“", "<|zh|>": "", "<|en|>": "", "<|yue|>": "",
    "<|ja|>": "", "<|ko|>": "", "<|nospeech|>": "", "<|HAPPY|>": "ğŸ˜Š", "<|SAD|>": "ğŸ˜”",
    "<|ANGRY|>": "ğŸ˜¡", "<|NEUTRAL|>": "", "<|BGM|>": "ğŸ¼", "<|Speech|>": "",
    "<|Applause|>": "ğŸ‘", "<|Laughter|>": "ğŸ˜€", "<|FEARFUL|>": "ğŸ˜°",
    "<|DISGUSTED|>": "ğŸ¤¢", "<|SURPRISED|>": "ğŸ˜®", "<|Cry|>": "ğŸ˜­", "<|EMO_UNKNOWN|>": "",
    "<|Sneeze|>": "ğŸ¤§", "<|Breath|>": "", "<|Cough|>": "ğŸ˜·", "<|Sing|>": "",
    "<|Speech_Noise|>": "", "<|withitn|>": "", "<|woitn|>": "", "<|GBG|>": "", "<|Event_UNK|>": "",
}
lang_dict = {
    "<|zh|>": "<|lang|>", "<|en|>": "<|lang|>", "<|yue|>": "<|lang|>",
    "<|ja|>": "<|lang|>", "<|ko|>": "<|lang|>", "<|nospeech|>": "<|lang|>",
}
emo_set = {"ğŸ˜Š", "ğŸ˜”", "ğŸ˜¡", "ğŸ˜°", "ğŸ¤¢", "ğŸ˜®"}
event_set = {"ğŸ¼", "ğŸ‘", "ğŸ˜€", "ğŸ˜­", "ğŸ¤§", "ğŸ˜·"}

def format_str(s):
    for sptk in emoji_dict:
        s = s.replace(sptk, emoji_dict[sptk])
    return s

def format_str_v2(s):
    sptk_dict = {}
    for sptk in emoji_dict:
        sptk_dict[sptk] = s.count(sptk)
        s = s.replace(sptk, "")
    emo = "<|NEUTRAL|>"
    for e in emo_dict:
        if sptk_dict[e] > sptk_dict[emo]:
            emo = e
    for e in event_dict:
        if sptk_dict[e] > 0:
            s = event_dict[e] + s
    s = s + emo_dict[emo]
    for emoji in emo_set.union(event_set):
        s = s.replace(" " + emoji, emoji)
        s = s.replace(emoji + " ", emoji)
    return s.strip()

def format_str_v3(s):
    def get_emo(s):
        return s[-1] if s[-1] in emo_set else None
    def get_event(s):
        return s[0] if s[0] in event_set else None
    s = s.replace("<|nospeech|><|Event_UNK|>", "â“")
    for lang in lang_dict:
        s = s.replace(lang, "<|lang|>")
    s_list = [format_str_v2(s_i).strip(" ") for s_i in s.split("<|lang|>")]
    new_s = " " + s_list[0]
    cur_ent_event = get_event(new_s)
    for i in range(1, len(s_list)):
        if len(s_list[i]) == 0:
            continue
        if get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) != None:
            s_list[i] = s_list[i][1:]
        cur_ent_event = get_event(s_list[i])
        if get_emo(s_list[i]) != None and get_emo(s_list[i]) == get_emo(new_s):
            new_s = new_s[:-1]
        new_s += s_list[i].strip().lstrip()
    new_s = new_s.replace("The.", " ")
    return new_s.strip()

def contains_chinese_english_number(s: str) -> bool:
    return bool(re.search(r'[\u4e00-\u9fffA-Za-z0-9]', s))


# --- æ¨¡å‹åŠ è½½ (æ— æ”¹åŠ¨) ---
sv_pipeline = pipeline(
    task='speaker-verification',
    model='iic/speech_eres2net_large_sv_zh-cn_3dspeaker_16k',
    model_revision='v1.0.0'
)
model_asr = AutoModel(
    model="iic/SenseVoiceSmall",
    trust_remote_code=True,
    remote_code="./model.py",
    device="cuda:0",
    disable_update=True
)
model_vad = AutoModel(
    model="fsmn-vad",
    model_revision="v2.0.4",
    disable_pbar=True,
    max_end_silence_time=500,
    disable_update=True,
)


# --- å¼‚æ­¥æ¨¡å‹æ¨ç†åŒ…è£…å‡½æ•° ---
async def async_vad_generate(chunk, cache_vad, chunk_size_ms):
    """å¼‚æ­¥VADæ¨ç†"""
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(
        thread_pool_executor,
        lambda: model_vad.generate(input=chunk, cache=cache_vad, is_final=False, chunk_size=chunk_size_ms)
    )

async def async_sv_pipeline(audio_pair):
    """å¼‚æ­¥è¯´è¯äººéªŒè¯"""
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(
        thread_pool_executor,
        lambda: sv_pipeline(audio_pair)
    )

async def async_asr_generate(audio, lang, cache, use_itn=False):
    """å¼‚æ­¥è¯­éŸ³è¯†åˆ«"""
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(
        thread_pool_executor,
        lambda: model_asr.generate(
            input=audio,
            cache=cache,
            language=lang.strip(),
            use_itn=use_itn,
            batch_size_s=60,
        )
    )

# --- ã€å·²åºŸå¼ƒã€‘æ—§çš„å…¨å±€è¯´è¯äººæ³¨å†Œé€»è¾‘ ---
# reg_spks_files = [
#     "speaker/speaker1_a_cn_16k.wav"
# ]
# def reg_spk_init(files): ...
# reg_spks = reg_spk_init(reg_spks_files)


# --- ã€æ”¹è¿›ã€‘åœ¨çº¿è¯´è¯äººæ—¥å¿—å‡½æ•° ---
def check_audio_quality(audio_segment, sample_rate=16000):
    """
    æ£€æŸ¥éŸ³é¢‘è´¨é‡ï¼Œç¡®ä¿é€‚åˆè¿›è¡Œè¯´è¯äººè¯†åˆ«
    
    Args:
        audio_segment (np.ndarray): éŸ³é¢‘ç‰‡æ®µ
        sample_rate (int): é‡‡æ ·ç‡
        
    Returns:
        bool: éŸ³é¢‘è´¨é‡æ˜¯å¦åˆæ ¼
    """
    if len(audio_segment) == 0:
        return False
    
    # æ£€æŸ¥éŸ³é¢‘é•¿åº¦
    duration_ms = len(audio_segment) / sample_rate * 1000
    if duration_ms < config.min_audio_length_ms or duration_ms > config.max_audio_length_ms:
        logger.debug(f"Audio duration {duration_ms:.1f}ms not suitable for speaker verification")
        return False
    
    # æ£€æŸ¥éŸ³é¢‘èƒ½é‡ - é™ä½è¦æ±‚
    energy = np.mean(np.abs(audio_segment))
    if energy < 0.005:  # é™ä½èƒ½é‡é˜ˆå€¼ï¼ˆåŸå€¼ï¼š0.01ï¼‰
        logger.debug(f"Audio energy {energy:.4f} too low for speaker verification")
        return False
    
    # æ£€æŸ¥éŸ³é¢‘æ–¹å·®ï¼ˆé¿å…é™éŸ³æˆ–å™ªå£°ï¼‰- é™ä½è¦æ±‚
    variance = np.var(audio_segment)
    if variance < 0.0005:  # é™ä½æ–¹å·®é˜ˆå€¼ï¼ˆåŸå€¼ï¼š0.001ï¼‰
        logger.debug(f"Audio variance {variance:.6f} too low for speaker verification")
        return False
    
    return True

async def diarize_speaker_online_improved_async(audio_segment, speaker_gallery, speaker_counter, sv_thr, 
                                               speaker_history=None, current_speaker=None):
    """
    æ”¹è¿›çš„å¼‚æ­¥åœ¨çº¿è¯´è¯äººæ—¥å¿—åˆ†æå‡½æ•°
    
    Args:
        audio_segment (np.ndarray): å½“å‰çš„è¯­éŸ³ç‰‡æ®µ
        speaker_gallery (dict): å½“å‰ä¼šè¯çš„å£°çº¹åº“ {speaker_id: reference_audio}
        speaker_counter (int): å½“å‰ä¼šè¯çš„è¯´è¯äººè®¡æ•°å™¨
        sv_thr (float): å£°çº¹æ¯”å¯¹çš„ç›¸ä¼¼åº¦é˜ˆå€¼
        speaker_history (list): è¯´è¯äººå†å²è®°å½• [(speaker_id, confidence, timestamp), ...]
        current_speaker (str): å½“å‰æ´»è·ƒçš„è¯´è¯äºº
        
    Returns:
        tuple: (è¯†åˆ«å‡ºçš„speaker_id, æ›´æ–°åçš„speaker_gallery, æ›´æ–°åçš„speaker_counter, 
                æ›´æ–°åçš„speaker_history, æ›´æ–°åçš„current_speaker)
    """
    current_time = time.time()
    
    # åˆå§‹åŒ–å†å²è®°å½•
    if speaker_history is None:
        speaker_history = []
    
    # æ£€æŸ¥éŸ³é¢‘è´¨é‡
    if not check_audio_quality(audio_segment):
        # éŸ³é¢‘è´¨é‡ä¸åˆæ ¼ï¼Œä½¿ç”¨å½“å‰è¯´è¯äººæˆ–é»˜è®¤å€¼
        if current_speaker:
            logger.debug(f"Using current speaker '{current_speaker}' due to poor audio quality")
            return current_speaker, speaker_gallery, speaker_counter, speaker_history, current_speaker
        else:
            return "å‘è¨€äºº", speaker_gallery, speaker_counter, speaker_history, current_speaker
    
    # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªè¯´è¯äºº
    if not speaker_gallery:
        speaker_counter += 1
        speaker_id = f"å‘è¨€äºº{speaker_counter}"
        speaker_gallery[speaker_id] = audio_segment
        speaker_history.append((speaker_id, 1.0, current_time))
        logger.info(f"First speaker detected. Assigning ID: {speaker_id}")
        return speaker_id, speaker_gallery, speaker_counter, speaker_history, speaker_id
    
    # ä¸å£°çº¹åº“ä¸­çš„æ‰€æœ‰è¯´è¯äººè¿›è¡Œå¹¶å‘æ¯”å¯¹
    comparison_tasks = []
    speaker_ids = list(speaker_gallery.keys())
    
    for spk_id in speaker_ids:
        ref_audio = speaker_gallery[spk_id]
        task = async_sv_pipeline([audio_segment, ref_audio])
        comparison_tasks.append((spk_id, task))
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰è¯´è¯äººéªŒè¯
    best_score = -1.0
    identified_speaker = None
    scores = {}
    
    try:
        for spk_id, task in comparison_tasks:
            try:
                res_sv = await task
                score = res_sv["score"]
                scores[spk_id] = score
                logger.debug(f"Comparing with '{spk_id}', score: {score:.4f}")
                if score > best_score:
                    best_score = score
                    identified_speaker = spk_id
            except Exception as e:
                logger.error(f"Error during speaker comparison with {spk_id}: {e}")
                continue
    except Exception as e:
        logger.error(f"Error during concurrent speaker verification: {e}")
        # å¦‚æœå¹¶å‘æ¯”å¯¹å¤±è´¥ï¼Œä½¿ç”¨å½“å‰è¯´è¯äººæˆ–é»˜è®¤å€¼
        if current_speaker:
            return current_speaker, speaker_gallery, speaker_counter, speaker_history, current_speaker
        else:
            return "å‘è¨€äºº", speaker_gallery, speaker_counter, speaker_history, current_speaker
    
    # åŠ¨æ€é˜ˆå€¼è°ƒæ•´ï¼šåŸºäºå†å²è®°å½•å’Œå½“å‰è¯´è¯äºº
    dynamic_threshold = sv_thr
    
    # å¦‚æœå½“å‰æœ‰æ´»è·ƒè¯´è¯äººï¼Œé™ä½åˆ‡æ¢é˜ˆå€¼
    if current_speaker and current_speaker in scores:
        current_score = scores[current_speaker]
        # å¦‚æœå½“å‰è¯´è¯äººåˆ†æ•°è¾ƒé«˜ï¼Œæé«˜åˆ‡æ¢é˜ˆå€¼
        if current_score > sv_thr * 0.8:
            dynamic_threshold = sv_thr * 1.1  # é™ä½é˜ˆå€¼æå‡å€æ•°ï¼ˆåŸå€¼ï¼š1.2ï¼‰
            logger.debug(f"Raised threshold to {dynamic_threshold:.3f} for current speaker '{current_speaker}'")
    
    # è¿ç»­æ€§åˆ¤æ–­ - é™ä½ä¸¥æ ¼ç¨‹åº¦
    if current_speaker and identified_speaker == current_speaker and best_score >= sv_thr * 0.6:  # é™ä½è¿ç»­æ€§é˜ˆå€¼ï¼ˆåŸå€¼ï¼š0.7ï¼‰
        # è¿ç»­è¯´è¯ï¼Œä¿æŒå½“å‰è¯´è¯äºº
        confidence = min(1.0, best_score)
        speaker_history.append((current_speaker, confidence, current_time))
        logger.info(f"Speaker continuity confirmed: '{current_speaker}' (score: {best_score:.4f})")
        return current_speaker, speaker_gallery, speaker_counter, speaker_history, current_speaker
    
    # æ£€æŸ¥æ˜¯å¦åŒ¹é…åˆ°å·²æœ‰è¯´è¯äºº
    if best_score >= dynamic_threshold:
        # åŒ¹é…åˆ°å·²æœ‰è¯´è¯äºº
        confidence = min(1.0, best_score)
        speaker_history.append((identified_speaker, confidence, current_time))
        logger.info(f"Matched existing speaker '{identified_speaker}' with score {best_score:.4f}")
        return identified_speaker, speaker_gallery, speaker_counter, speaker_history, identified_speaker
    
    # æ£€æŸ¥æ˜¯å¦åº”è¯¥åˆ›å»ºæ–°è¯´è¯äºº - é™ä½åˆ›å»ºæ–°è¯´è¯äººçš„é—¨æ§›
    # åªæœ‰å½“æ‰€æœ‰åˆ†æ•°éƒ½æ˜æ˜¾ä½äºé˜ˆå€¼æ—¶æ‰åˆ›å»ºæ–°è¯´è¯äºº
    all_scores_low = all(score < sv_thr * 0.7 for score in scores.values())  # é™ä½é—¨æ§›ï¼ˆåŸå€¼ï¼š0.8ï¼‰
    
    if all_scores_low:
        # åˆ›å»ºæ–°è¯´è¯äºº
        speaker_counter += 1
        new_speaker_id = f"å‘è¨€äºº{speaker_counter}"
        speaker_gallery[new_speaker_id] = audio_segment
        speaker_history.append((new_speaker_id, 0.8, current_time))
        logger.info(f"New speaker detected (all scores < {sv_thr * 0.7:.3f}). Assigning ID: {new_speaker_id}")
        return new_speaker_id, speaker_gallery, speaker_counter, speaker_history, new_speaker_id
    else:
        # åˆ†æ•°ä¸å¤Ÿé«˜ä½†ä¹Ÿä¸å¤Ÿä½ï¼Œä½¿ç”¨æœ€ä½³åŒ¹é…
        best_speaker = max(scores.items(), key=lambda x: x[1])[0]
        confidence = min(0.7, scores[best_speaker])
        speaker_history.append((best_speaker, confidence, current_time))
        logger.info(f"Using best match '{best_speaker}' with moderate confidence {scores[best_speaker]:.4f}")
        return best_speaker, speaker_gallery, speaker_counter, speaker_history, best_speaker

# ä¿ç•™åŸå‡½æ•°åä»¥å…¼å®¹ç°æœ‰ä»£ç ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
def diarize_speaker_online_improved(audio_segment, speaker_gallery, speaker_counter, sv_thr, 
                                   speaker_history=None, current_speaker=None):
    """
    æ”¹è¿›çš„åœ¨çº¿è¯´è¯äººæ—¥å¿—åˆ†æå‡½æ•°ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œä¿æŒå…¼å®¹æ€§ï¼‰
    """
    import time
    
    # åˆå§‹åŒ–å†å²è®°å½•
    if speaker_history is None:
        speaker_history = []
    current_time = time.time()
    
    # æ£€æŸ¥éŸ³é¢‘è´¨é‡
    if not check_audio_quality(audio_segment):
        # éŸ³é¢‘è´¨é‡ä¸åˆæ ¼ï¼Œä½¿ç”¨å½“å‰è¯´è¯äººæˆ–é»˜è®¤å€¼
        if current_speaker:
            logger.debug(f"Using current speaker '{current_speaker}' due to poor audio quality")
            return current_speaker, speaker_gallery, speaker_counter, speaker_history, current_speaker
        else:
            return "å‘è¨€äºº", speaker_gallery, speaker_counter, speaker_history, current_speaker
    
    # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªè¯´è¯äºº
    if not speaker_gallery:
        speaker_counter += 1
        speaker_id = f"å‘è¨€äºº{speaker_counter}"
        speaker_gallery[speaker_id] = audio_segment
        speaker_history.append((speaker_id, 1.0, current_time))
        logger.info(f"First speaker detected. Assigning ID: {speaker_id}")
        return speaker_id, speaker_gallery, speaker_counter, speaker_history, speaker_id
    
    # ä¸å£°çº¹åº“ä¸­çš„æ‰€æœ‰è¯´è¯äººè¿›è¡Œæ¯”å¯¹
    best_score = -1.0
    identified_speaker = None
    scores = {}
    
    for spk_id, ref_audio in speaker_gallery.items():
        try:
            res_sv = sv_pipeline([audio_segment, ref_audio])
            score = res_sv["score"]
            scores[spk_id] = score
            logger.debug(f"Comparing with '{spk_id}', score: {score:.4f}")
            if score > best_score:
                best_score = score
                identified_speaker = spk_id
        except Exception as e:
            logger.error(f"Error during speaker comparison with {spk_id}: {e}")
            continue
    
    # åŠ¨æ€é˜ˆå€¼è°ƒæ•´ï¼šåŸºäºå†å²è®°å½•å’Œå½“å‰è¯´è¯äºº
    dynamic_threshold = sv_thr
    
    # å¦‚æœå½“å‰æœ‰æ´»è·ƒè¯´è¯äººï¼Œé™ä½åˆ‡æ¢é˜ˆå€¼
    if current_speaker and current_speaker in scores:
        current_score = scores[current_speaker]
        # å¦‚æœå½“å‰è¯´è¯äººåˆ†æ•°è¾ƒé«˜ï¼Œæé«˜åˆ‡æ¢é˜ˆå€¼
        if current_score > sv_thr * 0.8:
            dynamic_threshold = sv_thr * 1.1  # é™ä½é˜ˆå€¼æå‡å€æ•°ï¼ˆåŸå€¼ï¼š1.2ï¼‰
            logger.debug(f"Raised threshold to {dynamic_threshold:.3f} for current speaker '{current_speaker}'")
    
    # è¿ç»­æ€§åˆ¤æ–­ - é™ä½ä¸¥æ ¼ç¨‹åº¦
    if current_speaker and identified_speaker == current_speaker and best_score >= sv_thr * 0.6:  # é™ä½è¿ç»­æ€§é˜ˆå€¼ï¼ˆåŸå€¼ï¼š0.7ï¼‰
        # è¿ç»­è¯´è¯ï¼Œä¿æŒå½“å‰è¯´è¯äºº
        confidence = min(1.0, best_score)
        speaker_history.append((current_speaker, confidence, current_time))
        logger.info(f"Speaker continuity confirmed: '{current_speaker}' (score: {best_score:.4f})")
        return current_speaker, speaker_gallery, speaker_counter, speaker_history, current_speaker
    
    # æ£€æŸ¥æ˜¯å¦åŒ¹é…åˆ°å·²æœ‰è¯´è¯äºº
    if best_score >= dynamic_threshold:
        # åŒ¹é…åˆ°å·²æœ‰è¯´è¯äºº
        confidence = min(1.0, best_score)
        speaker_history.append((identified_speaker, confidence, current_time))
        logger.info(f"Matched existing speaker '{identified_speaker}' with score {best_score:.4f}")
        return identified_speaker, speaker_gallery, speaker_counter, speaker_history, identified_speaker
    
    # æ£€æŸ¥æ˜¯å¦åº”è¯¥åˆ›å»ºæ–°è¯´è¯äºº - é™ä½åˆ›å»ºæ–°è¯´è¯äººçš„é—¨æ§›
    # åªæœ‰å½“æ‰€æœ‰åˆ†æ•°éƒ½æ˜æ˜¾ä½äºé˜ˆå€¼æ—¶æ‰åˆ›å»ºæ–°è¯´è¯äºº
    all_scores_low = all(score < sv_thr * 0.7 for score in scores.values())  # é™ä½é—¨æ§›ï¼ˆåŸå€¼ï¼š0.8ï¼‰
    
    if all_scores_low:
        # åˆ›å»ºæ–°è¯´è¯äºº
        speaker_counter += 1
        new_speaker_id = f"å‘è¨€äºº{speaker_counter}"
        speaker_gallery[new_speaker_id] = audio_segment
        speaker_history.append((new_speaker_id, 0.8, current_time))
        logger.info(f"New speaker detected (all scores < {sv_thr * 0.7:.3f}). Assigning ID: {new_speaker_id}")
        return new_speaker_id, speaker_gallery, speaker_counter, speaker_history, new_speaker_id
    else:
        # åˆ†æ•°ä¸å¤Ÿé«˜ä½†ä¹Ÿä¸å¤Ÿä½ï¼Œä½¿ç”¨æœ€ä½³åŒ¹é…
        best_speaker = max(scores.items(), key=lambda x: x[1])[0]
        confidence = min(0.7, scores[best_speaker])
        speaker_history.append((best_speaker, confidence, current_time))
        logger.info(f"Using best match '{best_speaker}' with moderate confidence {scores[best_speaker]:.4f}")
        return best_speaker, speaker_gallery, speaker_counter, speaker_history, best_speaker

# ä¿ç•™åŸå‡½æ•°åä»¥å…¼å®¹ç°æœ‰ä»£ç 
def diarize_speaker_online(audio_segment, speaker_gallery, speaker_counter, sv_thr):
    """
    å…¼å®¹æ€§åŒ…è£…å‡½æ•°ï¼Œè°ƒç”¨æ”¹è¿›çš„è¯´è¯äººè¯†åˆ«ç®—æ³•
    """
    result = diarize_speaker_online_improved(
        audio_segment, speaker_gallery, speaker_counter, sv_thr, 
        speaker_history=None, current_speaker=None
    )
    return result[0], result[1], result[2]  # åªè¿”å›å‰ä¸‰ä¸ªå€¼ä»¥ä¿æŒå…¼å®¹æ€§


async def asr_async(audio, lang, cache, use_itn=False):
    """å¼‚æ­¥è¯­éŸ³è¯†åˆ«å‡½æ•°"""
    start_time = time.time()
    result = await async_asr_generate(audio, lang, cache, use_itn)
    end_time = time.time()
    elapsed_time = end_time - start_time
    logger.debug(f"asr elapsed: {elapsed_time * 1000:.2f} milliseconds")
    return result

def asr(audio, lang, cache, use_itn=False):
    """ä¿æŒåŸæœ‰åŒæ­¥å‡½æ•°ä»¥å…¼å®¹å…¶ä»–åœ°æ–¹çš„è°ƒç”¨"""
    start_time = time.time()
    result = model_asr.generate(
        input=audio,
        cache=cache,
        language=lang.strip(),
        use_itn=use_itn,
        batch_size_s=60,
    )
    end_time = time.time()
    elapsed_time = end_time - start_time
    logger.debug(f"asr elapsed: {elapsed_time * 1000:.2f} milliseconds")
    return result


# --- FastAPI åº”ç”¨è®¾ç½® ---
app = FastAPI(lifespan=lifespan)
app.add_middleware(
    CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"],
)
@app.exception_handler(Exception)
async def custom_exception_handler(request: Request, exc: Exception):
    # ... (æ­¤å¤„çœç•¥å¼‚å¸¸å¤„ç†ä»£ç , ä¸æ‚¨åŸä»£ç ç›¸åŒ)
    logger.error("Exception occurred", exc_info=True)
    if isinstance(exc, HTTPException):
        status_code = exc.status_code
        message = exc.detail
    elif isinstance(exc, RequestValidationError):
        status_code = HTTP_422_UNPROCESSABLE_ENTITY
        message = "Validation error: " + str(exc.errors())
    else:
        status_code = 500
        message = "Internal server error: " + str(exc)
    
    return JSONResponse(
        status_code=status_code,
        content=TranscriptionResponse(
            code=status_code,
            msg=message, # ä¿®æ­£: åŸä»£ç æ­¤å¤„ä¸ºinfo, æ”¹ä¸ºmsgä»¥åŒ¹é…æ¨¡å‹å®šä¹‰
            data=""
        ).model_dump()
    )

class TranscriptionResponse(BaseModel):
    code: int
    msg: str # ä¿æŒä¸åŸå®šä¹‰ä¸€è‡´
    data: str

# ä¿®æ­£: ä¸Šæ–¹å¼‚å¸¸å¤„ç†å™¨ä¸­çš„contentåº”ä¸æ¨¡å‹å®šä¹‰åŒ¹é…
# class TranscriptionResponse(BaseModel):
#     code: int
#     msg: str  # å‡è®¾æ¨¡å‹å®šä¹‰æ˜¯msg
#     data: str


# --- ã€ä¼˜åŒ–åã€‘WebSocket ç«¯ç‚¹ ---
@app.websocket("/ws/transcribe")
async def websocket_endpoint(websocket: WebSocket):
    try:
        query_params = parse_qs(websocket.scope['query_string'].decode())
        sv = query_params.get('sv', ['false'])[0].lower() in ['true', '1', 't', 'y', 'yes']
        lang = query_params.get('lang', ['auto'])[0].lower()
        
        await websocket.accept()

        # --- ä¸ºæ¯ä¸ªä¼šè¯åˆå§‹åŒ–çŠ¶æ€ ---
        chunk_size = int(config.chunk_size_ms * config.sample_rate / 1000)
        
        # ä½¿ç”¨é«˜æ•ˆç¼“å†²åŒºï¼šä¸»ç¼“å†²åŒºç”¨äºæ¥æ”¶æ•°æ®ï¼ŒVADç¼“å†²åŒºå­˜å‚¨æ›´é•¿æ—¶é—´çš„éŸ³é¢‘ç”¨äºVADåˆ†æ
        audio_buffer = AudioBuffer(max_size=100)  # ä¸»éŸ³é¢‘æ¥æ”¶ç¼“å†²åŒº
        vad_buffer_size = config.sample_rate * 10  # 10ç§’çš„VADç¼“å†²åŒº
        audio_vad = CircularAudioBuffer(max_samples=vad_buffer_size)
        
        cache_vad = {}
        cache_asr = {}
        last_vad_beg = last_vad_end = -1
        offset = 0
        
        # ã€æ”¹è¿›ã€‘ä¸ºå½“å‰è¿æ¥åˆ›å»ºç‹¬ç«‹çš„å£°çº¹åº“å’Œè®¡æ•°å™¨
        speaker_gallery = {}  # key: "å‘è¨€äºº1", value: np.ndarray (å£°çº¹éŸ³é¢‘)
        speaker_counter = 0
        speaker_history = []  # è¯´è¯äººå†å²è®°å½•
        current_speaker = None  # å½“å‰æ´»è·ƒçš„è¯´è¯äºº
        
        buffer = b""
        logger.info(f"WebSocket session started with chunk_size={chunk_size}, vad_buffer_size={vad_buffer_size}")
        
        while True:
            data = await websocket.receive_bytes()
            buffer += data
            if len(buffer) < 2:
                continue
                
            # å°†å­—èŠ‚æ•°æ®è½¬æ¢ä¸ºæµ®ç‚¹æ•°éŸ³é¢‘æ•°æ®
            new_audio_data = np.frombuffer(
                buffer[:len(buffer) - (len(buffer) % 2)], 
                dtype=np.int16
            ).astype(np.float32) / 32767.0
            
            audio_buffer.append(new_audio_data)
            buffer = buffer[len(buffer) - (len(buffer) % 2):]
   
            # å¤„ç†éŸ³é¢‘chunk
            while len(audio_buffer) >= chunk_size:
                # ä»ä¸»ç¼“å†²åŒºè·å–ä¸€ä¸ªchunk
                chunk = audio_buffer.pop_front(chunk_size)
                
                # å°†chunkæ·»åŠ åˆ°VADç¼“å†²åŒº
                audio_vad.append(chunk)
                
                # ä½¿ç”¨å¼‚æ­¥VADæ¨ç†
                res = await async_vad_generate(chunk, cache_vad, config.chunk_size_ms)
                
                if len(res[0]["value"]):
                    for segment in res[0]["value"]:
                        if segment[0] > -1: last_vad_beg = segment[0]
                        if segment[1] > -1: last_vad_end = segment[1]
                        if last_vad_beg > -1 and last_vad_end > -1:
                            beg = int((last_vad_beg - offset) * config.sample_rate / 1000)
                            end = int((last_vad_end - offset) * config.sample_rate / 1000)
                            
                            # ç¡®ä¿ç´¢å¼•ä¸è¶…å‡ºVADç¼“å†²åŒºèŒƒå›´
                            vad_buffer_length = len(audio_vad)
                            if beg < vad_buffer_length and end > beg:
                                end = min(end, vad_buffer_length)
                                segment_length = end - beg
                                
                                segment_audio = audio_vad.get_range(beg, segment_length)
                                logger.info(f"[vad segment] audio_len: {len(segment_audio)}, beg: {beg}, end: {end}")

                                speaker_id = "å‘è¨€äºº" # é»˜è®¤ID
                                if sv and len(segment_audio) > 0:
                                    # ä½¿ç”¨æ”¹è¿›çš„å¼‚æ­¥è¯´è¯äººè¯†åˆ«ç®—æ³•
                                    try:
                                        speaker_id, speaker_gallery, speaker_counter, speaker_history, current_speaker = await diarize_speaker_online_improved_async(
                                            segment_audio, speaker_gallery, speaker_counter, config.sv_thr,
                                            speaker_history, current_speaker
                                        )
                                    except Exception as e:
                                        logger.error(f"Speaker verification error: {e}")
                                        speaker_id = "å‘è¨€äºº"
                                
                                # 2. è¿›è¡Œå¼‚æ­¥è¯­éŸ³è¯†åˆ«
                                try:
                                    result = await asr_async(segment_audio, lang.strip(), cache_asr, True)
                                    logger.info(f"asr response: {result}")
                                    
                                    if result is not None and contains_chinese_english_number(result[0]['text']):
                                        formatted_text = format_str_v3(result[0]['text'])
                                        
                                        # 3. åˆå¹¶è¯´è¯äººIDå’Œè¯†åˆ«æ–‡æœ¬
                                        final_data = f"[{speaker_id}]: {formatted_text}"
                                        
                                        response = TranscriptionResponse(
                                            code=0,
                                            msg=json.dumps(result[0], ensure_ascii=False),
                                            data=final_data
                                        )
                                        await websocket.send_json(response.model_dump())
                                        
                                except Exception as e:
                                    logger.error(f"ASR processing error: {e}")
                                
                                # æ¸…ç†å·²å¤„ç†çš„VADæ•°æ®ï¼ˆä¿ç•™ä¸€äº›é‡å ä»¥ç¡®ä¿è¿ç»­æ€§ï¼‰
                                overlap_samples = int(0.1 * config.sample_rate)  # 100msé‡å 
                                clear_length = max(0, end - overlap_samples)
                                if clear_length > 0:
                                    audio_vad.pop_front(clear_length)
                                    offset += clear_length / config.sample_rate * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                                
                                last_vad_beg = last_vad_end = -1
                                
    except WebSocketDisconnect:
        logger.info("WebSocket disconnected")
    except Exception as e:
        logger.error(f"Unexpected error: {e}\nCall stack:\n{traceback.format_exc()}")
        await websocket.close()
    finally:
        logger.info("Cleaned up resources after WebSocket disconnect")


# --- æœåŠ¡å¯åŠ¨ (æ— æ”¹åŠ¨) ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run the FastAPI app with a specified port.")
    parser.add_argument('--port', type=int, default=27000, help='Port number to run the FastAPI app on.')
    parser.add_argument('--certfile', type=str, default='path_to_your_certfile', help='SSL certificate file')
    parser.add_argument('--keyfile', type=str, default='path_to_your_keyfile', help='SSL key file')
    args = parser.parse_args()

    uvicorn.run(app, host="0.0.0.0", port=args.port)